# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tyfTzjYSpqSH80fsh3yNFMprBIrDVXXb
"""

!pip install topsis

!pip install nltk

import pandas as pd
import torch
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk import ngrams

# Define the models to evaluate
models = [
    "gpt2",
    "gpt2-medium",
    "openai-gpt",
    "facebook/bart-large",
    "sshleifer/tiny-gpt2",
    "microsoft/CodeGPT-small-py"
]

# Initialize an empty DataFrame to store the results
columns = ["Model", "BLEU_Score", "ROUGE_Score", "Perplexity", "TOPSIS_Score", "Rank"]
results_df = pd.DataFrame(columns=columns)

# Define a function to evaluate a model and calculate metrics
def evaluate_model(model_name, text_generator, tokenizer):
    # Sample input text
    input_text = "Today is a beautiful day and"
    # Calculate BLEU score
    reference = ["Today is a beautiful day. The sun is shining, and the birds are singing."]

    # Generate text using the model
    generated_text = text_generator(input_text, max_length=50, num_return_sequences=1, num_beams=5, no_repeat_ngram_size=2, return_tensors="pt")

    # Corrected key for accessing generated text
    hypothesis = tokenizer.decode(generated_text[0]["generated_token_ids"], skip_special_tokens=True)

# Tokenize reference and hypothesis sentences
    reference_tokens = [reference[0].split()]
    hypothesis_tokens = hypothesis.split()

# Calculate BLEU score
    bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=SmoothingFunction().method1)

    # Calculate ROUGE score
    reference_ngrams = [set(ngrams(reference[0].split(), 2))]
    hypothesis_ngrams = set(ngrams(hypothesis.split(), 2))
    overlap = sum(len(ngram) for ngram in hypothesis_ngrams.intersection(*reference_ngrams))
    rouge_score = overlap / len(reference_ngrams[0])

    # Calculate perplexity
    input_ids = tokenizer.encode(input_text, return_tensors="pt")
    with torch.no_grad():
        logits = model(input_ids).logits
    perplexity = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), input_ids.view(-1))

    return bleu_score, rouge_score, perplexity.item()

# Iterate through models and evaluate them
for model_name in models:
    # Load the model and tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Create a text generation pipeline
    text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

    # Evaluate the model and get metrics
    bleu_score, rouge_score, perplexity = evaluate_model(model_name, text_generator, tokenizer)

    # Calculate TOPSIS score manually without normalization
    topsis_score = (bleu_score * rouge_score) / perplexity

    # Store the results in the DataFrame
    results_df = results_df.append({
        "Model": model_name,
        "BLEU_Score": bleu_score,
        "ROUGE_Score": rouge_score,
        "Perplexity": perplexity,
        "TOPSIS_Score": topsis_score
    }, ignore_index=True)

# Add ranks to the DataFrame
results_df["Rank"] = results_df["TOPSIS_Score"].rank(ascending=False)

# Save the results to a CSV file
results_df.to_csv("102117161-TopsisTextGeneration-results.csv", index=False)

print("Evaluation and TOPSIS analysis completed. Results saved to 'model_evaluation_results.csv'.")

results_df

# TOPSIS SCORE COMPARISON
import pandas as pd
import matplotlib.pyplot as plt

results_df = pd.read_csv("/content/102117161-TopsisTextGeneration-results.csv")
plt.figure(figsize=(10, 6))
plt.plot(results_df["Model"], results_df["TOPSIS_Score"], marker='o', linestyle='-', color='b', label='TOPSIS Score')
plt.xlabel('Model')
plt.ylabel('TOPSIS Score')
plt.title('Comparison of Text Generation Models')
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# BAR GRAPH
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("/content/102117161-TopsisTextGeneration-results.csv")
df=df.drop(columns=['TOPSIS_Score','Rank'])

df.set_index("Model", inplace=True)
df.plot(kind="bar", figsize=(12, 6))
plt.title("Comparison of Models based on Evaluation Parameters")
plt.xlabel("Models")
plt.ylabel("Scores")
plt.xticks(rotation=45, ha="right")
plt.legend(title="Parameters")
plt.tight_layout()
plt.show()

# HEAT MAP
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

results_df = pd.read_csv("/content/102117161-TopsisTextGeneration-results.csv")
results_corr_df = results_df.drop(columns=["Model",'TOPSIS_Score','Rank'])

corr_matrix = results_corr_df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Between Evaluation Parameters")
plt.show()